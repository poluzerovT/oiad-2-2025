# ОИАД. Теория к лабораторной №3

## Типы признаков

* **Количественные/непрерывные** - бесконечное число значений или, по крайней мере, очень много. Например, рост человека, зарпалата.
* **Бинарные** - всего 2 значения. Например, пол: мальчик, девочка.
* **Категориальные/номинальные** - принимают конечное число значений. Значения нельзя упорядочить. Наример, уровни должности, грейды. (Старший специалист > стажера)
* **Порядковые/ранговые** - конечное число значений, можно упорядочить. Например, цвета, города (Нельзя сказать, что красный > зеленого).

## Преобразование признаков в численные
**Бинарные**
Одно из значений заменяют $1$, другое $0$.

**Порядковые**
Порядковые признаки можно просто пронумеровать. Это сохранит порядок среди значений.

| X                  |
| ------------------ |
| младший специалист |
| старший специалист |
| среднийспециалист  |
| босс               |

$\rightarrow$
| X   |
| --- |
| 1   |
| 3   |
| 2   |
| 4   |

*Какой недостаток у такого подхода?*

**Номинальные**
Под каждое уникальное значение признака создается столбец. У каждого объекта в столбец, отвечающий за соотвествующее значение, ставится $1$, в остальные - $0$.

| X       |
| ------- |
| Красный |
| Синий   |
| Красный |
| Зеленый |

$\rightarrow$

| Красный | Синий | Зеленый |
| ------- | ----- | ------- |
| 1       | 0     | 0       |
| 0       | 1     | 0       |
| 1       | 0     | 0       |
| 0       | 0     | 1       |

## Многомерная линейная регрессия

Матрица объекты-признаки $X$ размера $\ell \times n$, где $\ell$ - чило объектов выборке, а каждый оъект описывается $n$ признаками. Будем считать, что среди признаков есть константа.
Параметры модели $w$ - вектор стобец размера $n$.

Модель регрессии
$$
f(x, w) = \sum_{i=1}^{n} w_i x_i = Xw
$$

Функционал потерь
$$
Q(w) = \sum_{i=1}^{\ell} (f(x_i, w) - y_i)^2 = ||Xw - y||^2 \rightarrow \min_{w} 
$$

Градиент функционала потерь
$$
\frac{\partial Q}{\partial w} = 2X^T(Xw-y)
$$

**Аналитическое решение**
Для поиска точки минимума, приравниваем градиент к нулю.
$$
2X^T(Xw-y) = 0
$$

$$
X^TXw = X^Ty
$$

$$
w^* = (X^TX)^{-1}X^Ty
$$

*В каких случаях могут возникнуть проблемы с решением?*

**Численное решение**
Подобные оптимизационные задачи решаются градиентными методами. Основная идея состоит в итерационном движении от одной точки к другой в пространстве параметров модели. Направление движения определяется с помощью градиента функции. Градиент функции в некоторой точке указывает направление наискорейшего роста. Мы же хотим минимизировать функцию, поэтому направление выберем как минус градиент в этой точке. Размер шага будет определятся абсолютным значением градиента в точке и некоторым параметром.

## Градиентные методы
**Метод градиентного спуска**
Классический метод градиентного спуска. Другие методы модифицируют его, внося изменения в подход к определению размеру или направлению шага.

$w^{(0)}$ - начальное приближение
$\lambda$ - размер градиентного шага

$w^{(k+1)} = w^{(k)} - \lambda \cdot \frac{\partial Q}{\partial w}|_{w=w^{(k)}}$

Остановка просходит в случаях:
* градиент близок к нулю
* изменение параметров близко к нулю
* достигнуто ограничительное число итераций
  
**Стохастический градиентный спуск**
Идея состоит в расчете не точного значения градиента $\frac{\partial Q}{\partial w}|_{w=w^{(k)}}$ на всей выборке $X$ размера $\ell$, а оценке его значения по части выборки $\tilde{X} \subset X$ меньшего размера $\tilde{\ell} < \ell$.

*Какое преимущество имеет этот метод по сравнению с классическим?*

## Регуляризация
В случае сильной корреляции между признаками (столбцами матрицы $X$), обращение матрицы $(X^TX)^{-1}$ ведет к увеличению её абсолютных значений (а в случае полной линейной зависимости - вообще необратимости), и в результате веса модели становятся большими по модулю и неустойчивыми.
Бороться с этим эффектом можно с помощью регуляризации. В функционал ошибки добавляют слагаемое, пропорциональное норме весов. Таким образом можно получить различные модификации регрессионной модели. 

**Гребневая регрессия**
$$
Q_{L_2}(w) = ||Xw - y||^2 + \alpha ||w||_2^2 \rightarrow \min_{w}
$$

$$
||a||_2 = \sqrt{\sum_{i=1}^{n} a_i^2}
$$

Имеет аналитическое решение
$$
w^* = (X^TX + \alpha I)^{-1}X^Ty
$$
где $I$ - единичная матрица

Градиент
$$
\frac{\partial Q_{L_2}}{\partial w} = 2 X^T (Xw - y) + 2 \alpha w
$$


**LASSO (Least Absolute Shrinkage and Selection Operator)**
$$
Q_{L_1}(w) = ||Xw - y||^2 + \beta ||w||_1 \rightarrow \min_{w}
$$

$$
||a||_1 = \sum_{i=1}^{n} |a_i|
$$

Эта задача не имеет аналитического решения

$$
\frac{\partial Q_{L_1}}{\partial w} = 2 X^T (Xw - y) + \beta~sign(w)
$$

## Оценка обобщающей способности
Построение регрессионной модели и оценка ее качества на одних и тех же данных не покажет, сохранится ли её качество на новых данных.
Поэтому набор данных случайным образо разбивают на 2 выборки: 
* обучающую, на которой подбирают параметры модели
* тестовую, на которой оценивают качество
